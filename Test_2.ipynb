{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1e3056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "import contractions\n",
    "import unicodedata\n",
    "\n",
    "nltk.download('punkt')\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605092bb",
   "metadata": {},
   "source": [
    "### WIKI version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e32211ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_utf(text):\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\").replace('\\u201C', \"`\").replace('\\u201D', \"`\").replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    return text.decode('ascii')\n",
    "\n",
    "def tokenise_sentences(text):\n",
    "    #convert utf-8 characters to normal characters\n",
    "    text = convert_utf(text)\n",
    "    \n",
    "    #make lowercase\n",
    "    text_lowercase = text.lower()\n",
    "\n",
    "    #fix contractions\n",
    "    expanded_text = contractions.fix(text_lowercase)\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(expanded_text)\n",
    "\n",
    "    data = []\n",
    "       \n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        words = [ word for word in words if word.isalpha() and word not in stopwords.words('english')]\n",
    "\n",
    "        data.append(words)\n",
    "       \n",
    "        \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64b3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(vector_size=200, min_count=1, sg=0)\n",
    "model.save(\"./test_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97d62bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api in c:\\users\\chad\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\chad\\anaconda3\\lib\\site-packages (from wikipedia-api) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chad\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chad\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chad\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chad\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia-api\n",
    "import wikipediaapi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06cf0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'ChadBrowneProjectt/1.0 (david.freeborn@nulondon.ac.uk)'\n",
    "\n",
    "# 2) language to specify language mutation. It has to be one of supported languages e.g. 'en'\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5094a362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0516e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e04a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(data, update=False)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./test_wiki_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceb4dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_page_content(page_title):\n",
    "    page = wiki_wiki.page(page_title)\n",
    "    return page.text\n",
    "\n",
    "# Example usage\n",
    "page_title = 'polar bear' # PICK A DIFFERENT EXAMPLE TO ME!\n",
    "\n",
    "# Proceed with the rest of your preprocessing and analysis...\n",
    "text = get_wiki_page_content(page_title)\n",
    "\n",
    "data = tokenise_sentences(text)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./test_wiki_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c75b7f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bears', 0.8788639307022095), ('polar', 0.8672313690185547), ('bear', 0.8658287525177002), ('ice', 0.8594094514846802), ('century', 0.8549824953079224)]\n"
     ]
    }
   ],
   "source": [
    "print(  model.wv.most_similar('man', topn=5)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da26ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfe08e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_page_content(page_title):\n",
    "    page = wiki_wiki.page(page_title)\n",
    "    return page.text\n",
    "\n",
    "# Example usage\n",
    "page_title = 'world' # PICK A DIFFERENT EXAMPLE TO ME!\n",
    "\n",
    "# Proceed with the rest of your preprocessing and analysis...\n",
    "text = get_wiki_page_content(page_title)\n",
    "\n",
    "data = tokenise_sentences(text)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./test_wiki_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44ee8f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bears', 0.952337920665741), ('world', 0.9511792659759521), ('one', 0.9474523663520813), ('like', 0.9448035955429077), ('polar', 0.944631814956665)]\n"
     ]
    }
   ],
   "source": [
    "print(  model.wv.most_similar('man', topn=5)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2200df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_page_content(page_title):\n",
    "    page = wiki_wiki.page(page_title)\n",
    "    return page.text\n",
    "\n",
    "# Example usage\n",
    "page_title = 'earth' # PICK A DIFFERENT EXAMPLE TO ME!\n",
    "\n",
    "# Proceed with the rest of your preprocessing and analysis...\n",
    "text = get_wiki_page_content(page_title)\n",
    "\n",
    "data = tokenise_sentences(text)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./test_wiki_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be3b913b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('early', 0.9574585556983948), ('km', 0.9570662379264832), ('current', 0.9566758871078491), ('century', 0.9565338492393494), ('many', 0.956532895565033)]\n"
     ]
    }
   ],
   "source": [
    "print(  model.wv.most_similar('man', topn=5)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5044ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_page_content(page_title):\n",
    "    page = wiki_wiki.page(page_title)\n",
    "    return page.text\n",
    "\n",
    "# Example usage\n",
    "page_title = 'human' # PICK A DIFFERENT EXAMPLE TO ME!\n",
    "\n",
    "# Proceed with the rest of your preprocessing and analysis...\n",
    "text = get_wiki_page_content(page_title)\n",
    "\n",
    "data = tokenise_sentences(text)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./test_wiki_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbd004bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_page_content(page_title):\n",
    "    page = wiki_wiki.page(page_title)\n",
    "    return page.text\n",
    "\n",
    "# Example usage\n",
    "page_title = 'technology' # PICK A DIFFERENT EXAMPLE TO ME!\n",
    "\n",
    "# Proceed with the rest of your preprocessing and analysis...\n",
    "text = get_wiki_page_content(page_title)\n",
    "\n",
    "data = tokenise_sentences(text)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./test_wiki_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c979673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('early', 0.9963712692260742), ('human', 0.996264636516571), ('many', 0.9962593913078308), ('century', 0.9962536692619324), ('life', 0.9962183833122253)]\n"
     ]
    }
   ],
   "source": [
    "print(  model.wv.most_similar('man', topn=5)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2d2cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_page_content(page_title):\n",
    "    page = wiki_wiki.page(page_title)\n",
    "    return page.text\n",
    "\n",
    "# Example usage\n",
    "page_title = 'gender' # PICK A DIFFERENT EXAMPLE TO ME!\n",
    "\n",
    "# Proceed with the rest of your preprocessing and analysis...\n",
    "text = get_wiki_page_content(page_title)\n",
    "\n",
    "data = tokenise_sentences(text)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./test_wiki_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4abaab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gender', 0.9996615648269653), ('human', 0.9996519684791565), ('many', 0.999646782875061), ('use', 0.9996381998062134), ('also', 0.999638020992279)]\n"
     ]
    }
   ],
   "source": [
    "print(  model.wv.most_similar('man', topn=5)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79ada97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_page_content(page_title):\n",
    "    page = wiki_wiki.page(page_title)\n",
    "    return page.text\n",
    "\n",
    "# Example usage\n",
    "page_title = 'tree' # PICK A DIFFERENT EXAMPLE TO ME!\n",
    "\n",
    "# Proceed with the rest of your preprocessing and analysis...\n",
    "text = get_wiki_page_content(page_title)\n",
    "\n",
    "data = tokenise_sentences(text)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./test_wiki_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3620dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('often', 0.999715268611908), ('gender', 0.9997141361236572), ('many', 0.9997103214263916), ('use', 0.9997071027755737), ('human', 0.9997043609619141)]\n"
     ]
    }
   ],
   "source": [
    "print(  model.wv.most_similar('man', topn=5)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8919d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab6303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61cfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947677e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a744b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d282879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f481d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to model -- needs to be alter for wikipeadia\n",
    "data = tokenise_sentences(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38cbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
